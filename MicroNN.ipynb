{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MicroNN\n",
        "\n",
        "A minimal neural network implementation from scratch in NumPy that learns to classify handwritten digits using forward propagation, backpropagation, and gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [dataset](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)\n",
        "\n",
        "Author: [Kevin Thomas](mailto:ket189@pitt.edu)\n",
        "\n",
        "License: MIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (2.4.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (3.10.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\assem.kevinthomas\\onedrive\\documents\\data-science\\mnist\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O3LiFxWFwc1v"
      },
      "outputs": [],
      "source": [
        "class MicroNN:\n",
        "    \"\"\"A minimal 2-layer neural network for digit classification.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : int\n",
        "        Number of input features (default: 784 for 28x28 images).\n",
        "    hidden_size : int\n",
        "        Number of neurons in the hidden layer (default: 10).\n",
        "    output_size : int\n",
        "        Number of output classes (default: 10 for digits 0-9).\n",
        "\n",
        "    Shapes\n",
        "    ------\n",
        "    - Input: (input_size, num_samples)\n",
        "    - Output: (output_size, num_samples)\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Uses ReLU activation for hidden layer and softmax for output.\n",
        "    - Weights initialized uniformly in range [-0.5, 0.5].\n",
        "    - Trained using vanilla gradient descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int = 784, hidden_size: int = 10, output_size: int = 10):\n",
        "        # Store layer sizes\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        # Initialize weights and biases for hidden layer\n",
        "        self.w1 = np.random.rand(hidden_size, input_size) - 0.5\n",
        "        self.b1 = np.random.rand(hidden_size, 1) - 0.5\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.w2 = np.random.rand(output_size, hidden_size) - 0.5\n",
        "        self.b2 = np.random.rand(output_size, 1) - 0.5\n",
        "        # Cached activations for backpropagation\n",
        "        self.z1 = None\n",
        "        self.a1 = None\n",
        "        self.z2 = None\n",
        "        self.a2 = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Perform forward propagation through the network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : np.ndarray\n",
        "            Input features of shape (input_size, num_samples).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Output probabilities of shape (output_size, num_samples).\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        - Caches intermediate values (z1, a1, z2, a2) for backpropagation.\n",
        "        \"\"\"\n",
        "        # Hidden layer: linear + ReLU\n",
        "        self.z1 = self.w1.dot(x) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        # Output layer: linear + softmax\n",
        "        self.z2 = self.w2.dot(self.a1) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def relu(self, z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply ReLU activation function element-wise.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : np.ndarray\n",
        "            Pre-activation values of any shape.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            max(0, z) applied element-wise, same shape as input.\n",
        "        \"\"\"\n",
        "        return np.maximum(z, 0)\n",
        "\n",
        "    def softmax(self, z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply softmax activation to convert logits to probabilities.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : np.ndarray\n",
        "            Logits of shape (num_classes, num_samples).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Probabilities summing to 1 along axis 0, same shape as input.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        - Subtracts max for numerical stability before exponentiating.\n",
        "        \"\"\"\n",
        "        # Shift by max for numerical stability\n",
        "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "    def one_hot(self, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Convert integer labels to one-hot encoded matrix.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        y : np.ndarray\n",
        "            Integer labels of shape (num_samples,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            One-hot matrix of shape (num_classes, num_samples).\n",
        "        \"\"\"\n",
        "        one_hot_y = np.zeros((self.output_size, y.size))\n",
        "        one_hot_y[y, np.arange(y.size)] = 1\n",
        "        return one_hot_y\n",
        "\n",
        "    def backward(self, x: np.ndarray, y: np.ndarray) -> tuple:\n",
        "        \"\"\"Compute gradients via backpropagation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : np.ndarray\n",
        "            Input features of shape (input_size, num_samples).\n",
        "        y : np.ndarray\n",
        "            Integer labels of shape (num_samples,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            (dw1, db1, dw2, db2) gradients for weights and biases.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        - Uses cross-entropy loss with softmax output.\n",
        "        - Gradients averaged over batch size.\n",
        "        \"\"\"\n",
        "        m = y.size\n",
        "        # Convert labels to one-hot\n",
        "        one_hot_y = self.one_hot(y)\n",
        "        # Output layer gradients\n",
        "        dz2 = self.a2 - one_hot_y\n",
        "        dw2 = (1 / m) * dz2.dot(self.a1.T)\n",
        "        db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)\n",
        "        # Hidden layer gradients\n",
        "        dz1 = self.w2.T.dot(dz2) * self.relu_derivative(self.z1)\n",
        "        dw1 = (1 / m) * dz1.dot(x.T)\n",
        "        db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)\n",
        "        return dw1, db1, dw2, db2\n",
        "\n",
        "    def relu_derivative(self, z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute derivative of ReLU activation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : np.ndarray\n",
        "            Pre-activation values of any shape.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            1 where z > 0, else 0, same shape as input.\n",
        "        \"\"\"\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "    def update(self, grads: tuple, lr: float):\n",
        "        \"\"\"Update weights and biases using gradient descent.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        grads : tuple\n",
        "            (dw1, db1, dw2, db2) gradients from backward pass.\n",
        "        lr : float\n",
        "            Learning rate (step size for gradient descent).\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        - Applies: weight = weight - lr * gradient\n",
        "        \"\"\"\n",
        "        dw1, db1, dw2, db2 = grads\n",
        "        # Update hidden layer parameters\n",
        "        self.w1 -= lr * dw1\n",
        "        self.b1 -= lr * db1\n",
        "        # Update output layer parameters\n",
        "        self.w2 -= lr * dw2\n",
        "        self.b2 -= lr * db2\n",
        "\n",
        "    def train(self, x: np.ndarray, y: np.ndarray, lr: float = 0.1, epochs: int = 1000, print_every: int = 20):\n",
        "        \"\"\"Train the network using gradient descent.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : np.ndarray\n",
        "            Training features of shape (input_size, num_samples).\n",
        "        y : np.ndarray\n",
        "            Training labels of shape (num_samples,).\n",
        "        lr : float\n",
        "            Learning rate (default: 0.1).\n",
        "        epochs : int\n",
        "            Number of training iterations (default: 1000).\n",
        "        print_every : int\n",
        "            Print progress every N epochs (default: 20).\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        - Performs full-batch gradient descent.\n",
        "        - Prints training accuracy at specified intervals.\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.forward(x)\n",
        "            # Backward pass\n",
        "            grads = self.backward(x, y)\n",
        "            # Update parameters\n",
        "            self.update(grads, lr)\n",
        "            # Print progress\n",
        "            if epoch % print_every == 0:\n",
        "                preds = self.predict(x)\n",
        "                acc = self.accuracy(preds, y)\n",
        "                print(f\"Epoch {epoch:4d} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict class labels for input samples.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : np.ndarray\n",
        "            Input features of shape (input_size, num_samples).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Predicted labels of shape (num_samples,).\n",
        "        \"\"\"\n",
        "        probs = self.forward(x)\n",
        "        return np.argmax(probs, axis=0)\n",
        "\n",
        "    def accuracy(self, preds: np.ndarray, y: np.ndarray) -> float:\n",
        "        \"\"\"Calculate classification accuracy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        preds : np.ndarray\n",
        "            Predicted labels of shape (num_samples,).\n",
        "        y : np.ndarray\n",
        "            True labels of shape (num_samples,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Fraction of correct predictions (0.0 to 1.0).\n",
        "        \"\"\"\n",
        "        return np.sum(preds == y) / y.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zssj7kcX24PE",
        "outputId": "c5277007-ab2a-4504-adf8-c43660dda6e3"
      },
      "outputs": [],
      "source": [
        "def load_data(path: str, train_split: float = 0.8):\n",
        "    \"\"\"Load and prepare MNIST data for training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "        Path to the CSV file containing MNIST data.\n",
        "    train_split : float\n",
        "        Fraction of data to use for training (default: 0.8).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x_train : np.ndarray\n",
        "        Training features of shape (784, num_train_samples).\n",
        "    y_train : np.ndarray\n",
        "        Training labels of shape (num_train_samples,).\n",
        "    x_val : np.ndarray\n",
        "        Validation features of shape (784, num_val_samples).\n",
        "    y_val : np.ndarray\n",
        "        Validation labels of shape (num_val_samples,).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Shuffles data before splitting.\n",
        "    - Normalizes pixel values to [0, 1] range.\n",
        "    - Transposes features so samples are columns.\n",
        "    \"\"\"\n",
        "    # Load CSV data\n",
        "    data = np.array(pd.read_csv(path))\n",
        "    n = len(data)\n",
        "    # Shuffle data\n",
        "    np.random.shuffle(data)\n",
        "    # Split into train and validation\n",
        "    split_idx = int(train_split * n)\n",
        "    train_data = data[:split_idx, :]\n",
        "    val_data = data[split_idx:, :]\n",
        "    # Extract features and labels (first column is label)\n",
        "    x_train = train_data[:, 1:].T / 255.0\n",
        "    y_train = train_data[:, 0]\n",
        "    x_val = val_data[:, 1:].T / 255.0\n",
        "    y_val = val_data[:, 0]\n",
        "    return x_train, y_train, x_val, y_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNWuThBS7I2u",
        "outputId": "4679f75c-b5a2-408d-96ad-52b35b9bf74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 48000\n",
            "Validation samples: 12000\n",
            "Epoch    0 | Accuracy: 0.1441\n",
            "Epoch   50 | Accuracy: 0.5475\n",
            "Epoch  100 | Accuracy: 0.6928\n",
            "Epoch  150 | Accuracy: 0.7504\n",
            "Epoch  200 | Accuracy: 0.7846\n",
            "Epoch  250 | Accuracy: 0.8068\n",
            "Epoch  300 | Accuracy: 0.8216\n",
            "Epoch  350 | Accuracy: 0.8333\n",
            "Epoch  400 | Accuracy: 0.8429\n",
            "Epoch  450 | Accuracy: 0.8503\n",
            "\n",
            "Validation Accuracy: 0.8530\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# LOAD DATA\n",
        "# =============================================================================\n",
        "x_train, y_train, x_val, y_val = load_data(\"mnist_train.csv\")\n",
        "print(f\"Training samples: {y_train.size}\")\n",
        "print(f\"Validation samples: {y_val.size}\")\n",
        "\n",
        "# =============================================================================\n",
        "# CREATE AND TRAIN MODEL\n",
        "# =============================================================================\n",
        "model = MicroNN(input_size=784, hidden_size=10, output_size=10)\n",
        "model.train(x_train, y_train, lr=0.1, epochs=500, print_every=50)\n",
        "\n",
        "# =============================================================================\n",
        "# EVALUATE ON VALIDATION SET\n",
        "# =============================================================================\n",
        "val_preds = model.predict(x_val)\n",
        "val_acc = model.accuracy(val_preds, y_val)\n",
        "print(f\"\\nValidation Accuracy: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: 2 | Actual: 2\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST SINGLE PREDICTION\n",
        "# =============================================================================\n",
        "sample_idx = 42\n",
        "pred = model.predict(x_val[:, sample_idx, None])[0]\n",
        "actual = y_val[sample_idx]\n",
        "print(f\"Predicted: {pred} | Actual: {actual}\")\n",
        "\n",
        "# Display image\n",
        "plt.imshow(x_val[:, sample_idx].reshape(28, 28), cmap=\"gray\")\n",
        "plt.title(f\"Predicted: {pred}, Actual: {actual}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
